{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Credit Default Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive namespace is empty.\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1683635021560
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Handle to the workspace\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLClient\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Authentication package\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01midentity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultAzureCredential\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'azure'"
     ]
    }
   ],
   "source": [
    "# Handle to the workspace\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "# Authentication package\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635022080
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=\"6df20682-3fc5-48fd-98e3-927f1ffaf36b\",\n",
    "    resource_group_name=\"antoine.naudy-rg\",\n",
    "    workspace_name=\"formation\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635022866
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "# Name assigned to the compute cluster\n",
    "cpu_compute_target = \"PDS6\"#\"cpu-cluster\"\n",
    "\n",
    "try:\n",
    "    # let's see if the compute target already exists\n",
    "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
    "    print(\n",
    "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\n",
    "    )\n",
    "\n",
    "except Exception:\n",
    "    print(\"Creating a new cpu compute target...\")\n",
    "\n",
    "    # Let's create the Azure ML compute object with the intended parameters\n",
    "    cpu_cluster = AmlCompute(\n",
    "        name=cpu_compute_target,\n",
    "        # Azure ML Compute is the on-demand VM service\n",
    "        type=\"amlcompute\",\n",
    "        # VM Family\n",
    "        size=\"STANDARD_DS3_V2\",\n",
    "        # Minimum running nodes when there is no job running\n",
    "        min_instances=0,\n",
    "        # Nodes in cluster\n",
    "        max_instances=4,\n",
    "        # How many seconds will the node running after the job termination\n",
    "        idle_time_before_scale_down=180,\n",
    "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
    "        tier=\"Dedicated\",\n",
    "    )\n",
    "    print(\n",
    "         f\"AMLCompute with name {cpu_cluster.name} will be created, with compute size {cpu_cluster.size}\"\n",
    "          )\n",
    "    # Now, we pass the object to MLClient's create_or_update method\n",
    "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635023381
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create a job environment\n",
    "import os\n",
    "\n",
    "dependencies_dir = \"./dependencies\"\n",
    "os.makedirs(dependencies_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%writefile {dependencies_dir}/conda.yml\n",
    "# Give the requirement for the virtual environment\n",
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - numpy=1.21.2\n",
    "  - pip=21.2.4\n",
    "  - scikit-learn=0.24.2\n",
    "  - scipy=1.7.1\n",
    "  - pandas>=1.1,<1.2\n",
    "  - pip:\n",
    "    - inference-schema[numpy-support]==1.3.0\n",
    "    - xlrd==2.0.1\n",
    "    - mlflow== 1.26.1\n",
    "    - azureml-mlflow==1.42.0\n",
    "    - psutil>=5.8,<5.9\n",
    "    - tqdm>=4.59,<4.60\n",
    "    - ipykernel~=6.0\n",
    "    - matplotlib\n",
    "    - lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635024730
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create the virtual environment\n",
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "custom_env_name = \"aml-scikit-learn\"\n",
    "\n",
    "pipeline_job_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for Credit Card Defaults pipeline\",\n",
    "    tags={\"scikit-learn\": \"0.24.2\"},\n",
    "    conda_file=os.path.join(dependencies_dir, \"conda.yml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\",\n",
    ")\n",
    "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Part 1 : Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Cleaned dataset df was retrieved from \"https://www.kaggle.com/code/brandonsfick/group-2-v01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635025362
    }
   },
   "outputs": [],
   "source": [
    "# pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635026021
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# pip install ydata_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635026613
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import generic libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635027502
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"df.csv\").sample(10000)#[['DAYS_ID_PUBLISH','DAYS_BIRTH','TARGET']].sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635028070
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Describe the data\n",
    "#from ydata_profiling import ProfileReport\n",
    "#profile = ProfileReport(df, minimal=True)\n",
    "#profile.to_file(\"report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635029027
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corr_matrix = df.iloc[:,0:10].corr()\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635029609
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# List of correlation pairs in descending order\n",
    "corr_matrix = df.corr()\n",
    "corr_series = corr_matrix.unstack()\n",
    "sorted_corr = corr_series.sort_values(ascending=False)\n",
    "sorted_corr = sorted_corr[sorted_corr != 1]\n",
    "print(sorted_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635030200
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Drop highly correlated variables - threshold 90% correlation\n",
    "df = df.drop(columns=['AMT_RECIVABLE_MEAN','AMT_TOTAL_RECEIVABLE_MEAN'])\n",
    "df = df.drop(columns=['AMT_RECEIVABLE_PRINCIPAL_MEAN','DAYS_ENTRY_PAYMENT_MEAN','NAME_CONTRACT_STATUS_Active_MEAN'])\n",
    "df=df.drop(columns=['DAYS_EMPLOYED_PERCENT','AMT_PAYMENT_CURRENT_MEAN','AMT_INST_MIN_REGULARITY_MEAN'])\n",
    "df=df.drop(columns=['CNT_DRAWINGS_POS_CURRENT_MEAN','CNT_INSTALMENT_FUTURE_MEAN','DAYS_ENDDATE_FACT_SUM'])\n",
    "df=df.drop(columns=['REGION_RATING_CLIENT_W_CITY','AMT_DRAWINGS_CURRENT_MEAN','DEF_60_CNT_SOCIAL_CIRCLE'])\n",
    "df=df.drop(columns=['MONTHS_BALANCE_MEAN_x','Date_Rank_MEAN','DAYS_ENDDATE_FACT_MIN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Part 2 : Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Part 3 : Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635030985
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 3-1 : Preprocessing : RobustScaler and SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 3-1-1 RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635031513
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries for preprocessing\n",
    "import time\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import auc, roc_curve, accuracy_score, roc_auc_score \n",
    "from sklearn.metrics import det_curve\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635032370
    }
   },
   "outputs": [],
   "source": [
    "# Normalize data and split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "X = df.drop(columns='TARGET').values\n",
    "#X = scaler.fit_transform(X) _> no normalization\n",
    "y = df['TARGET'].values\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 3-1-2 SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635032901
    }
   },
   "outputs": [],
   "source": [
    "#pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635033740
    }
   },
   "outputs": [],
   "source": [
    "# Model use for SMOTE testing\n",
    "model = DecisionTreeClassifier() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635034382
    }
   },
   "outputs": [],
   "source": [
    "# First option : borderline-SMOTE for imbalanced dataset\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import BorderlineSMOTE, SVMSMOTE\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "%pylab inline\n",
    "# summarize class distribution\n",
    "counter = Counter(y_train)\n",
    "print(counter)\n",
    "# transform the dataset\n",
    "oversample = BorderlineSMOTE()\n",
    "X_, y_ = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y_)\n",
    "print(counter)\n",
    "\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    " row_ix = where(y_ == label)[0]\n",
    " pyplot.scatter(X_[row_ix, 0], X_[row_ix, 1], label=str(label))\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "# prediction\n",
    "clf = GridSearchCV(model,\n",
    "            param_grid={},\n",
    "            cv=5,\n",
    "            scoring='roc_auc',\n",
    "            verbose=2)\n",
    "clf.fit(X_,y_)\n",
    "\n",
    "# Train scoring\n",
    "print('Train results: ','\\nModel: ', clf.best_estimator_, '\\nBest parameters: ', clf.best_params_, '\\nBest ROC AUC: %.2f' % clf.best_score_)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "#fpr, fnr, thresholds = det_curve(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred, normalize='all').ravel()\n",
    "\n",
    "# Test scoring\n",
    "print('Test results: ')\n",
    "print('Cost (10fn+fp): {:.2f}'.format(10*fn+fp))\n",
    "print('AUC: %.2f' % roc_auc_score(y_test, y_pred))\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635035243
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Second option : borderline-SMOTE with SVM for imbalanced dataset\n",
    "\n",
    "%pylab inline\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "# transform the dataset\n",
    "oversample = SVMSMOTE()\n",
    "X_, y_ = oversample.fit_resample(X_train, y_train)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y_)\n",
    "print(counter)\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    " row_ix = where(y_ == label)[0]\n",
    " pyplot.scatter(X_[row_ix, 0], X_[row_ix, 1], label=str(label))\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "# Fitting model\n",
    "clf = GridSearchCV(model,param_grid={},cv=5,scoring='roc_auc',verbose=2)\n",
    "clf.fit(X_,y_)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Train scoring\n",
    "print('Train results: ','\\nModel: ', clf.best_estimator_, '\\nBest parameters: ', clf.best_params_, '\\nBest ROC AUC: %.2f' % clf.best_score_)\n",
    "y_pred = clf.predict(X_test)\n",
    "#fpr, fnr, thresholds = det_curve(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred, normalize='all').ravel()\n",
    "\n",
    "# Test scoring\n",
    "print('Test results: ')\n",
    "print('Cost (10fn+fp): {:.2f}'.format(10*fn+fp))\n",
    "#print('Cost (10fnr+fpr): {:.2f}'.format(10*fnr[1]+fpr[1]))\n",
    "print('AUC: %.2f' % roc_auc_score(y_test, y_pred))\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose BorderlineSMOTE() over borderline-SMOTE with SVM. the first Borderline has a better scores with the baseline model DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635035913
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# borderline-SMOTE for imbalanced dataset (data are already normalized)\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "%pylab inline\n",
    "# summarize class distribution\n",
    "counter = Counter(y_train)\n",
    "print(counter)\n",
    "# transform the dataset\n",
    "oversample = BorderlineSMOTE()\n",
    "X_, y_ = oversample.fit_resample(X_train, y_train)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y_)\n",
    "print(counter)\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    " row_ix = where(y_ == label)[0]\n",
    " pyplot.scatter(X_[row_ix, 0], X_[row_ix, 1], label=str(label))\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 3-2 : Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635036691
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, recall_score, f1_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635037216
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Custom score function\n",
    "from sklearn.metrics import make_scorer, det_curve\n",
    "\n",
    "def cost_function(y_test, y_pred):\n",
    "    fpr, fnr, thresholds = det_curve(y_test, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred, normalize='all').ravel()\n",
    "    return 10*fn + fp\n",
    "\n",
    "custom_scorer = make_scorer(cost_function, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635037811
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Confusion matrix function\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def confmat(y_test, y_pred):\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred, normalize='all')\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred, normalize='all').ravel()\n",
    "    print('fp : ',fp,'\\ntn : ', fn)\n",
    "\n",
    "    # Change figure size and increase dpi for better resolution\n",
    "    plt.figure(figsize=(8,6), dpi=100)\n",
    "    # Scale up the size of all text\n",
    "    sns.set(font_scale = 1.1)\n",
    "\n",
    "    # Plot Confusion Matrix using Seaborn heatmap()\n",
    "    # Parameters:\n",
    "    # first param - confusion matrix in array format   \n",
    "    # annot = True: show the numbers in each heatmap cell\n",
    "    # fmt = 'd': show numbers as integers. \n",
    "    ax = sns.heatmap(conf_matrix, annot=True, fmt='.2g')\n",
    "\n",
    "    # set x-axis label and ticks. \n",
    "    ax.set_xlabel(\"Predicted\", fontsize=14, labelpad=20)\n",
    "    ax.xaxis.set_ticklabels(['0', '1'])\n",
    "\n",
    "    # set y-axis label and ticks\n",
    "    ax.set_ylabel(\"Actual\", fontsize=14, labelpad=20)\n",
    "    ax.yaxis.set_ticklabels(['0', '1'])\n",
    "\n",
    "    # set plot title\n",
    "    ax.set_title(\"Confusion Matrix for credit default\", fontsize=14, pad=20)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683635038736
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Definition of the models to train and test\n",
    "\n",
    "from scipy.stats import uniform as sp_randFloat\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "# Instanciation des différents modèles\n",
    "\n",
    "LOGREG_clf = LogisticRegression() \n",
    "RFC_clf= RandomForestClassifier()\n",
    "DUMMY_clf = DummyClassifier()\n",
    "LGBM_clf=LGBMClassifier()\n",
    "\n",
    "# Estimators list\n",
    "\n",
    "estimators = [(\"DummyClassifier\", DUMMY_clf),    \n",
    "              (\"LightGBM\",LGBM_clf), \n",
    "              (\"RandomForestClassifier\", RFC_clf),               \n",
    "              (\"LogisticRegression\", LOGREG_clf)                \n",
    "             ]\n",
    "\n",
    "# Hyperparameters range for the estimators \n",
    "\n",
    "LOGREG_rand_params= [{'max_iter':sp_randint(500, 5000),\n",
    "                      'C': np.logspace(-4, 4, 20),\n",
    "                     }]\n",
    "    \n",
    "RFC_rand_params= [{'n_estimators' :[100,500],\n",
    "                   'max_depth':[3,5,None],\n",
    "                   'max_features':sp_randint(1,3),\n",
    "                   'criterion':['gini'], \n",
    "                   'bootstrap':[True,False],\n",
    "                   'min_samples_leaf':sp_randint(1,4)                   \n",
    "                  }]\n",
    "\n",
    "LGBM_rand_params=[{'nthread':[2,6],\n",
    "                   'n_estimators':[300,700,1000],\n",
    "                   'learning_rate':sp_randFloat(0.02,0.2),\n",
    "                   'num_leaves':[30,35],\n",
    "                   'max_depth':[5,10,15],\n",
    "                   'min_child_weight':[35,40],\n",
    "                  }]\n",
    "            \n",
    "Dummy_rand_params= [{'strategy':['stratified']\n",
    "                    }]      \n",
    "\n",
    "# Hyperparameters dictionary used for RandomizedSearchCV\n",
    "\n",
    "params_list_RndSearchCV =  {'DummyClassifier': Dummy_rand_params,\n",
    "                            'LightGBM':LGBM_rand_params,\n",
    "                            'RandomForestClassifier': RFC_rand_params,\n",
    "                            'LogisticRegression': LOGREG_rand_params\n",
    "                           } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 3-3 : Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683623249718
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Train/fit models and display best model evaluations, graphs and scores\n",
    "from sklearn.metrics import classification_report, roc_auc_score, f1_score, accuracy_score\n",
    "\n",
    "result_table = []\n",
    "\n",
    "for (name, est) in (estimators):\n",
    "    # Cross validation with RandomizedSearchCV\n",
    "\n",
    "    score = df.shape[0]\n",
    "    model=[]\n",
    "    Names=[]\n",
    "    \n",
    "    # paramètre CV\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)    # n_splits=10, n_repeats=3\n",
    "    \n",
    "    # Instance RandomizedSearchCV\n",
    "    clf = RandomizedSearchCV(est, \n",
    "                                param_distributions=params_list_RndSearchCV[name],\n",
    "                                n_iter=10, \n",
    "                                cv=cv, \n",
    "                                scoring=custom_scorer, \n",
    "                                n_jobs=-1,    \n",
    "                                random_state=42,\n",
    "                                refit=\"custom_scorer\"\n",
    "                                )  \n",
    "\n",
    "    # Fit\n",
    "    clf.fit(X_, y_)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = clf.best_estimator_.predict(X_test)\n",
    "\n",
    "    # Display results\n",
    "    print(\"model : \", clf.best_estimator_)\n",
    "    confmat(y_test, y_pred)\n",
    "    print(classification_report (y_test, y_pred))\n",
    "    print(\"custom score train : \",(-clf.best_score_).round(2))\n",
    "    print(\"custom score test : \",cost_function(y_test, y_pred))\n",
    "    print('###################################################################################')\n",
    "        \n",
    "    fpr, tpr, threshold = det_curve(y_test, y_pred)\n",
    "    result_table.append({'models':clf.best_estimator_, \n",
    "                                        'fpr':fpr,\n",
    "                                        'tpr':tpr,\n",
    "                                        'auc':roc_auc_score(y_test, y_pred),\n",
    "                                        'F1':f1_score(y_test, y_pred), \n",
    "                                        'Custom Cost train':(-clf.best_score_).round(2),\n",
    "                                        'Custom Cost test' :cost_function(y_test, y_pred),\n",
    "                                        'Accuracy':accuracy_score(y_test, y_pred)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683623250343
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "result_table = pd.DataFrame(result_table)\n",
    "result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683623250946
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Enregistrement du meilleur modèle (score maison mini)\n",
    "index = np.argmin(result_table['Custom Cost test'])   \n",
    "       \n",
    "Model_name = result_table.iloc[index, 0]\n",
    "Model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683623261600
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import DetCurveDisplay, RocCurveDisplay\n",
    "\n",
    "#fig, [ax_roc, ax_det] = plt.subplots(1, 2, figsize=(11, 5))\n",
    "fig, ax_roc = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "for clf in result_table.iloc[:,0]:\n",
    "    clf.fit(X_, y_)\n",
    "\n",
    "    RocCurveDisplay.from_estimator(clf, X_test, y_test, ax=ax_roc, name=clf)\n",
    "    #DetCurveDisplay.from_estimator(clf, X_test, y_test, ax=ax_det, name=clf)\n",
    "\n",
    "ax_roc.set_title(\"Receiver Operating Characteristic (ROC) curves\")\n",
    "#ax_det.set_title(\"Detection Error Tradeoff (DET) curves\")\n",
    "\n",
    "ax_roc.grid(linestyle=\"--\")\n",
    "#ax_det.grid(linestyle=\"--\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1,1.02))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683623265659
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Best Threshold and cost with best model\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
    "model = Model_name\n",
    "# entraînez votre modèle de classification binaire\n",
    "model.fit(X_, y_)\n",
    "\n",
    "# obtenez les scores de probabilité pour votre ensemble de données de validation ou de test\n",
    "y_scores = model.predict_proba(X_test)[:, 1]\n",
    "range_th = np.linspace(0, 1, 101)\n",
    "range_co = []\n",
    "# appliquez différents seuils de classification aux scores de probabilité\n",
    "for threshold in range_th:\n",
    "    y_pred = (y_scores >= threshold).astype(int)\n",
    "\n",
    "    # évaluez la performance de chaque prédiction binaire en utilisant des mesures d'évaluation\n",
    "    range_co.append(cost_function(y_test, y_pred))\n",
    "    best_th = range_th[argmin(range_co)]\n",
    "print('Best model : ', model,'\\nBest Cost on test sample): ', min(range_co),'\\nBest Threshold : ',range_th[argmin(range_co)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 3-4 : Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683623266221
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# La liste des variables hors target\n",
    "feature_names = df.columns.tolist()\n",
    "feature_names.remove('TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683623269728
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Le meilleur modèle entraîné\n",
    "Best_clf = Model_name\n",
    "Best_clf.fit(X_, y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683623270279
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# La visualisation des important feature pour RandomForest\n",
    "if str(Model_name).startswith('RandomForest') == True :\n",
    "    feature_importance = Best_clf.coef_\n",
    "    sorted_idx = np.argsort(feature_importance) # np.argsort(feature_importance)[-6:-1]\n",
    "    sorted_idx = sorted_idx[:10]\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(range(len(sorted_idx)), np.array(df.columns)[sorted_idx])\n",
    "    plt.title('Feature Importance')\n",
    "\n",
    "    # La liste des features importantes\n",
    "    imp_feat = df_imp_best['Feature'].unique()\n",
    "    imp_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683623271169
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# La visualisation des important feature pour une régression logistique\n",
    "if str(Model_name).startswith('Logistic') == True :\n",
    "    feature_importance = abs(Best_clf.coef_[0])\n",
    "    sorted_idx = np.argsort(feature_importance) # np.argsort(feature_importance)[-6:-1]\n",
    "    sorted_idx = sorted_idx[:10]\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(range(len(sorted_idx)), np.array(df.columns)[sorted_idx])\n",
    "    plt.title('Feature Importance')\n",
    "\n",
    "    # Le dictionnaire des features importance du plus petit au plus grand\n",
    "    dic_imp = dict(zip(np.array(df.columns)[sorted_idx],feature_importance[sorted_idx]))\n",
    "    df_imp_best = pd.DataFrame(list(dic_imp.items()), columns = ['Feature', 'Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683623271734
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if str(Model_name).startswith('LGBM') :\n",
    "    import lightgbm\n",
    "\n",
    "    from lightgbm import LGBMClassifier\n",
    "\n",
    "    lgbm_model = LGBMClassifier()\n",
    "\n",
    "    lgbm_model.fit(X_, y_)\n",
    "\n",
    "    y_pred = lgbm_model.predict(X_test)\n",
    "\n",
    "\n",
    "    # Vizualisation\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    %matplotlib inline\n",
    "\n",
    "    def plotImp(model, X , num = 20, fig_size = (40, 20)):\n",
    "        feature_imp = pd.DataFrame({'Value':model.feature_importances_,'Feature':feature_names})\n",
    "        plt.figure(figsize=fig_size)\n",
    "        sns.set(font_scale = 5)\n",
    "        sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n",
    "                                                            ascending=False)[0:num])\n",
    "        plt.title('LightGBM Features (avg over folds)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('lgbm_importances-01.png')\n",
    "        plt.show()\n",
    "\n",
    "    plotImp(lgbm_model,X_,num = 10, fig_size = (40, 20))\n",
    "\n",
    "    # Le dictionnaire des features importance du plus petit au plus grand\n",
    "    feature_imp = pd.DataFrame({'Value':lgbm_model.feature_importances_,'Feature':feature_names})\n",
    "    df_imp_best = feature_imp.sort_values(by=\"Value\",ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Réentraînement du meilleur modèle sur les 10 features les plus importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683623272343
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# La liste des features importantes\n",
    "imp_feat = df_imp_best['Feature'].unique()\n",
    "imp_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683623273005
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_ = pd.read_csv('df.csv')[0:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683623273608
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Le dataset réduit aux features importantes\n",
    "feat = imp_feat.tolist()+['TARGET']\n",
    "df_i = df_.loc[:,feat]\n",
    "X_i = df_i.drop(columns = 'TARGET').values\n",
    "y_i = df_i['TARGET'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683623276190
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Split du dataset\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_i,y_i,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683623276240
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Entraînement du modéle\n",
    "Model_name.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683623276283
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Vérification du score\n",
    "y_te_pred = Model_name.predict(X_te)\n",
    "y_te_pred = (y_te_pred >= best_th)\n",
    "tn, fp, fn, tp = confusion_matrix(y_te, y_te_pred, normalize='all').ravel()\n",
    "\n",
    "# Test scoring\n",
    "print('Test results: ')\n",
    "print('Cost (10fn+fp): {:.2f}'.format(10*fn+fp))\n",
    "print('AUC: %.2f' % roc_auc_score(y_test, y_pred))\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Part 4 : Recording model and experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683623276329
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Recording model with pickle\n",
    "import pickle\n",
    "pickle.dump(Model_name,open(\"Best_predictor.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683623276411
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Recording experiment\n",
    "import os\n",
    "\n",
    "record_dir = \"./rd\"\n",
    "os.makedirs(record_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%writefile {record_dir}/main.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "#from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Start Logging\n",
    "    mlflow.start_run()\n",
    "\n",
    "    # enable autologging\n",
    "    mlflow.sklearn.autolog()\n",
    "\n",
    "    ###################\n",
    "    #<prepare the data>\n",
    "    ###################\n",
    "        \n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    df = pd.read_csv('df.csv')\n",
    "    X = df.drop(columns='TARGET').values\n",
    "    X = scaler.fit_transform(X)\n",
    "    y = df['TARGET'].values\n",
    "    \n",
    "    registered_model_name = 'LGBM'\n",
    "       \n",
    "    #LGBM_clf = LGBMClassifier(learning_rate=0.0330103185970559, max_depth=5,\n",
    "    #           min_child_weight=40, n_estimators=700, nthread=6, num_leaves=35)\n",
    "    \n",
    "    LGBM_clf = LogisticRegression(C=0.00026366508987303583, max_iter=2891)\n",
    "    LGBM_clf.fit(X,y)\n",
    "\n",
    "    mlflow.log_metric(\"num_samples\", df.shape[0])\n",
    "    mlflow.log_metric(\"num_features\", df.shape[1] - 1)\n",
    "\n",
    "    ##########################\n",
    "    #<save and register model with mlflow>\n",
    "    ##########################\n",
    "    # Registering the model to the workspace\n",
    "    print(\"Registering the model via MLFlow\")\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=LGBM_clf,\n",
    "        registered_model_name=registered_model_name,#args.registered_model_name,\n",
    "        artifact_path=registered_model_name#args.registered_model_name,\n",
    "    )\n",
    "\n",
    "    # Saving the model to a file\n",
    "    mlflow.sklearn.save_model(\n",
    "        sk_model=LGBM_clf,\n",
    "        path=os.path.join(registered_model_name, \"trained_model_PDS7_lgbm\"),\n",
    "    )\n",
    "    ###########################\n",
    "    #</save and register model>\n",
    "    ###########################\n",
    "    \n",
    "   \n",
    "\n",
    "    # Stop Logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683623276465
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input\n",
    "\n",
    "registered_model_name = \"credit_defaults_model_PDS7\"\n",
    "\n",
    "job = command(\n",
    "    inputs=dict(\n",
    "        data=Input(\n",
    "            type=\"uri_file\",\n",
    "            path=\"df.csv\" #\"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\",\n",
    "        ),\n",
    "        #test_train_ratio=0.2,\n",
    "        #learning_rate=0.25,\n",
    "        registered_model_name=registered_model_name,\n",
    "    ),\n",
    "    code=\"./rd/\",  # location of source code\n",
    "    command=\"python main.py --data ${{inputs.data}} --registered_model_name ${{inputs.registered_model_name}}\",\n",
    "    environment=\"aml-scikit-learn@latest\",\n",
    "    compute=\"PDS6\",\n",
    "    experiment_name=\"train_model_credit_default_prediction_PDS7\",\n",
    "    display_name=\"credit_default_prediction_PDS7\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683623276515
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ml_client.create_or_update(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Part 5 : Deploying model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683623276567
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
